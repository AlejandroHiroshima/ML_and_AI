{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b68d9cc",
   "metadata": {},
   "source": [
    "## Gemini API intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b1490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make predictions or decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# behövs ingen loaddotenv()\n",
    "# hittar GOOGLE_API_KEY av sig själv ändå\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29018619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some data engineering jokes, structured in short points:\n",
      "\n",
      "*   **Why did the ETL job fail?** It had *unresolved dependencies* – just like my last relationship.\n",
      "\n",
      "*   My favorite data engineering mantra: **\"It worked on my local machine!\"** (spoken right before deploying to production).\n",
      "\n",
      "*   What's the difference between a data lake and a data swamp? **Governance.**\n",
      "\n",
      "*   A data scientist walks into a bar and asks for a \"real-time, fully historized, cleansed data feed.\" The data engineer at the bar just sighs.\n",
      "\n",
      "*   What's a data engineer's favorite fairy tale? **\"The Self-Documenting Pipeline.\"**\n",
      "\n",
      "*   How do you know a data pipeline is truly robust? It only breaks in *production*, never staging.\n",
      "\n",
      "*   Schema drift isn't a bug; it's a **surprise feature** that appears every Tuesday.\n",
      "\n",
      "*   Debugging a data pipeline is like being a detective, but all the suspects are lying, and half the evidence is missing.\n",
      "\n",
      "*   The only thing \"real-time\" about most data engineering projects is the **panic** when something goes wrong.\n",
      "\n",
      "*   What does ETL really stand for? **Expect Troubles, Lament.**\n"
     ]
    }
   ],
   "source": [
    "def ask_llm(prompt, model= \"gemini-2.5-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "response= ask_llm(prompt =\"Give me som data engineering jokes, structure it in short points\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d093e5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04bae824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bc72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys() # samma som ovan cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "493077ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7cbe1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HttpResponse(\n",
       "  headers=<dict len=11>\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.sdk_http_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e576dbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Candidate(\n",
       "   content=Content(\n",
       "     parts=[\n",
       "       Part(\n",
       "         text=\"\"\"Here are some data engineering jokes, structured in short points:\n",
       " \n",
       " *   **Why did the ETL job fail?** It had *unresolved dependencies* – just like my last relationship.\n",
       " \n",
       " *   My favorite data engineering mantra: **\"It worked on my local machine!\"** (spoken right before deploying to production).\n",
       " \n",
       " *   What's the difference between a data lake and a data swamp? **Governance.**\n",
       " \n",
       " *   A data scientist walks into a bar and asks for a \"real-time, fully historized, cleansed data feed.\" The data engineer at the bar just sighs.\n",
       " \n",
       " *   What's a data engineer's favorite fairy tale? **\"The Self-Documenting Pipeline.\"**\n",
       " \n",
       " *   How do you know a data pipeline is truly robust? It only breaks in *production*, never staging.\n",
       " \n",
       " *   Schema drift isn't a bug; it's a **surprise feature** that appears every Tuesday.\n",
       " \n",
       " *   Debugging a data pipeline is like being a detective, but all the suspects are lying, and half the evidence is missing.\n",
       " \n",
       " *   The only thing \"real-time\" about most data engineering projects is the **panic** when something goes wrong.\n",
       " \n",
       " *   What does ETL really stand for? **Expect Troubles, Lament.**\"\"\"\n",
       "       ),\n",
       "     ],\n",
       "     role='model'\n",
       "   ),\n",
       "   finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "   index=0\n",
       " )]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4798b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are some data engineering jokes, structured in short points:\\n\\n*   **Why did the ETL job fail?** It had *unresolved dependencies* – just like my last relationship.\\n\\n*   My favorite data engineering mantra: **\"It worked on my local machine!\"** (spoken right before deploying to production).\\n\\n*   What\\'s the difference between a data lake and a data swamp? **Governance.**\\n\\n*   A data scientist walks into a bar and asks for a \"real-time, fully historized, cleansed data feed.\" The data engineer at the bar just sighs.\\n\\n*   What\\'s a data engineer\\'s favorite fairy tale? **\"The Self-Documenting Pipeline.\"**\\n\\n*   How do you know a data pipeline is truly robust? It only breaks in *production*, never staging.\\n\\n*   Schema drift isn\\'t a bug; it\\'s a **surprise feature** that appears every Tuesday.\\n\\n*   Debugging a data pipeline is like being a detective, but all the suspects are lying, and half the evidence is missing.\\n\\n*   The only thing \"real-time\" about most data engineering projects is the **panic** when something goes wrong.\\n\\n*   What does ETL really stand for? **Expect Troubles, Lament.**'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04be77",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "- basic unit of text for LLMs\n",
    "- can be as short as one character or as long as one word\n",
    "\n",
    "- tokens used for billing\n",
    "\n",
    "gemini free tier:\n",
    "- requests per minute (RPM): 10\n",
    "- tokens per minute (TPM): 250 000\n",
    "- requests per day (RPD): 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae47838a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=263,\n",
       "  prompt_token_count=13,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=13\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1604,\n",
       "  total_token_count=1880\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75441b3c",
   "metadata": {},
   "source": [
    "## Thinking\n",
    "- hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffdc12da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some data engineering jokes, structured in short points:\n",
      "\n",
      "*   **Schema Joke:**\n",
      "    *   What did the data engineer say to the empty table?\n",
      "    *   \"You need to define your purpose... and maybe a primary key.\"\n",
      "\n",
      "*   **ETL Problem:**\n",
      "    *   Why did the ETL job break up with the data lake?\n",
      "    *   It felt like it was doing all the work, and the lake was just... there.\n",
      "\n",
      "*   **Data Quality:**\n",
      "    *   A data engineer, a data scientist, and a business analyst walk into a bar.\n",
      "    *   The data engineer says, \"This data is a mess!\"\n",
      "    *   The data scientist asks, \"Can we still model it?\"\n",
      "    *   The business analyst asks, \"Is this the latest version?\"\n",
      "\n",
      "*   **Pipelines:**\n",
      "    *   My data pipeline is like my life:\n",
      "    *   It's constantly running, occasionally failing, and I'm not always sure what's going through it.\n",
      "\n",
      "*   **Debugging:**\n",
      "    *   How many data engineers does it take to change a lightbulb?\n",
      "    *   None, they'll just write a script to automatically detect and replace the bulb, but it will fail on the staging environment.\n",
      "\n",
      "*   **Spark:**\n",
      "    *   Why did the data engineer prefer Spark over Hadoop?\n",
      "    *   It gave them a bigger *cluster* of ideas.\n",
      "\n",
      "*   **Production vs. Dev:**\n",
      "    *   What's the difference between a data engineer in dev and one in production?\n",
      "    *   One is wondering *if* it will break, the other is wondering *when* it will break.\n",
      "\n",
      "*   **Legacy Systems:**\n",
      "    *   What's a data engineer's favorite type of horror movie?\n",
      "    *   One about maintaining a system built on Access 97.\n",
      "\n",
      "*   **SQL Optimization:**\n",
      "    *   A good data engineer can write a SQL query that runs in milliseconds.\n",
      "    *   A great data engineer can tell you *why* it should.\n",
      "\n",
      "*   **Documentation:**\n",
      "    *   What's the last thing a data engineer adds to a project?\n",
      "    *   A comment that says `// TODO: Add documentation`.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=\"Give me som data engineering jokes, structure it in short points\",\n",
    "        config= types.GenerateContentConfig(thinking_config=types.ThinkingConfig(thinking_budget=0))\n",
    "    )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87f2b247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=484,\n",
       "  prompt_token_count=13,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=13\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=497\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07eda97",
   "metadata": {},
   "source": [
    "## System instruction\n",
    "- hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deb265a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a concise explanation of OOP and dunder methods in Python:\n",
      "\n",
      "**OOP (Object-Oriented Programming)**\n",
      "\n",
      "*   **Core Idea:**  Organizes code around \"objects\" that combine data (attributes) and behavior (methods).\n",
      "*   **Key Principles:**\n",
      "    *   **Encapsulation:** Bundling data and methods that operate on that data within a class, hiding internal details.\n",
      "    *   **Inheritance:** Creating new classes (subclasses) based on existing classes (superclasses), inheriting their attributes and methods and extending them.\n",
      "    *   **Polymorphism:**  The ability of objects of different classes to respond to the same method call in their own way.\n",
      "    *   **Abstraction:** Simplifying complex systems by modeling classes appropriate to the problem.\n",
      "\n",
      "**Dunder Methods (Magic Methods)**\n",
      "\n",
      "*   **What they are:** Special methods in Python that start and end with double underscores (e.g., `__init__`, `__str__`).\n",
      "*   **Purpose:** Define how objects behave with built-in Python operators and functions.  They allow you to customize object creation, string representation, comparisons, arithmetic operations, and more.\n",
      "*   **Example:**\n",
      "    *   `__init__(self, ...)`:  The constructor, called when an object is created.\n",
      "    *   `__str__(self)`:  Returns a string representation of the object (used by `print()` and `str()`).\n",
      "    *   `__add__(self, other)`:  Defines the behavior of the `+` operator.\n",
      "\n",
      "**In short:** OOP is a way to structure your code, and dunder methods let you customize how your objects interact with Python's core functionality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert in Python programming, you will always provide idiomatic code, i.e. pythonic code. \n",
    "So when you see my code or my question, be very critical but answer in a short and concise way. \n",
    "Also be constructive to help me improve.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "explain OOP and dunder methods\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt,\n",
    "        config= types.GenerateContentConfig(\n",
    "            system_instruction=system_instruction\n",
    "            # thinking_config=types.ThinkingConfig(thinking_budget=0))\n",
    "    ))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9c50f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=364,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=364\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=68,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=68\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=432\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "metadata= response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ee036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.candidates_token_count = 364\n",
      "metadata.prompt_token_count = 68\n",
      "metadata.total_token_count = 432\n"
     ]
    }
   ],
   "source": [
    "print(f\"{metadata.candidates_token_count = }\") # output\n",
    "print(f\"{metadata.prompt_token_count = }\") # input + system instruction\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "154ba755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 43)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.split()), len(system_instruction.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88734a3",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "- creates randomness of output -> 'creative'\n",
    "- is a hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fe4c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fluffy gray rabbit hopped through the meadow, its nose twitching as it searched for clover. Its soft fur blended seamlessly with the surrounding rocks and shadows, providing excellent camouflage. With a flick of its white tail, it disappeared into the tall grass, leaving only a rustle behind.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 3 sentence about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=story,\n",
    "        config= types.GenerateContentConfig(\n",
    "            temperature=0\n",
    "            # system_instruction=system_instruction\n",
    "            # thinking_config=types.ThinkingConfig(thinking_budget=0))\n",
    "    ))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "873d7580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit hopped through the tall grass, its nose twitching as it scanned for danger. Its fur, a soft blend of silver and charcoal, camouflaged it perfectly against the earthy tones of the forest floor. With a final, cautious glance, it darted into the underbrush, disappearing into the shadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=story,\n",
    "        config= types.GenerateContentConfig(\n",
    "            temperature=2.0\n",
    "            # system_instruction=system_instruction\n",
    "            # thinking_config=types.ThinkingConfig(thinking_budget=0))\n",
    "    ))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b243b",
   "metadata": {},
   "source": [
    "## Multimodal input\n",
    "- input text and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfefbce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a close-up photo of a fluffy, gray rabbit wearing a miniature white and black cap that resembles a Swedish student graduation cap (studentmössa). A festive blue and yellow ribbon is draped over its back as it rests on a gray carpet.\n"
     ]
    }
   ],
   "source": [
    "text_input = \"Describe this image shortly\"\n",
    "image_input = {\"mime_type\": \"image/png\", \"data\": open(\"bella.png\", 'rb').read()}\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    contents=dict(\n",
    "        parts=[dict(text = text_input), dict(inline_data = image_input)]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e808a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-and-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
